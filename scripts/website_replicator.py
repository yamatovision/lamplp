#!/usr/bin/env python3
"""
Website Replicator - URLã‚’æŒ‡å®šã—ã¦ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®å®Œå…¨ãªãƒ­ãƒ¼ã‚«ãƒ«ãƒ¬ãƒ—ãƒªã‚«ã‚’ä½œæˆã™ã‚‹ãƒ„ãƒ¼ãƒ«

ä½¿ç”¨æ–¹æ³•:
    python3 website_replicator.py https://example.com
"""

import os
import sys
import re
import json
import time
import argparse
import requests
from urllib.parse import urljoin, urlparse
from datetime import datetime
import shutil


class WebsiteReplicator:
    """ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚¿ãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, url, output_dir=None):
        self.url = url.rstrip('/')
        self.domain = urlparse(url).netloc
        self.output_dir = output_dir or f"replica_{self.domain}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.downloaded = set()
        self.failed = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'html_files': 0,
            'css_files': 0,
            'js_files': 0,
            'images': 0,
            'fonts': 0,
            'other': 0,
            'total_size': 0
        }
    
    def replicate(self):
        """ãƒ¡ã‚¤ãƒ³ã®è¤‡è£½å‡¦ç†"""
        print(f"\nğŸš€ Website Replicator v1.0")
        print(f"ğŸ“ Target URL: {self.url}")
        print(f"ğŸ“ Output Directory: {self.output_dir}")
        print("=" * 60)
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: HTMLã‚’å–å¾—
        print("\n[1/7] ğŸ“„ Fetching HTML...")
        html_content = self.fetch_html()
        if not html_content:
            print("âŒ Failed to fetch HTML. Exiting.")
            return False
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒªã‚½ãƒ¼ã‚¹ã‚’æŠ½å‡º
        print("\n[2/7] ğŸ” Extracting resources...")
        resources = self.extract_resources(html_content)
        self.save_resource_list(resources)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        print("\n[3/7] ğŸ“¥ Downloading resources...")
        self.download_all_resources(resources)
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‘ã‚¹ã‚’å¤‰æ›
        print("\n[4/7] ğŸ”„ Converting paths...")
        html_content = self.convert_paths(html_content)
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: å¤±æ•—ã—ãŸãƒªã‚½ãƒ¼ã‚¹ã‚’ä¿®æ­£
        print("\n[5/7] ğŸ”§ Fixing missing resources...")
        self.fix_missing_resources()
        
        # ã‚¹ãƒ†ãƒƒãƒ—6: å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™çš„åŒ–
        print("\n[6/7] ğŸ“¦ Staticizing dynamic content...")
        html_content = self.staticize_content(html_content)
        
        # ã‚¹ãƒ†ãƒƒãƒ—7: æœ€çµ‚çš„ãªHTMLã‚’ä¿å­˜
        print("\n[7/7] ğŸ’¾ Saving final HTML...")
        self.save_html(html_content)
        
        # çµæœã‚’è¡¨ç¤º
        self.show_results()
        
        # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        self.create_helper_files()
        
        return True
    
    def fetch_html(self):
        """HTMLã‚’å–å¾—"""
        try:
            response = self.session.get(self.url, timeout=30)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            self.stats['html_files'] = 1
            self.stats['total_size'] += len(response.content)
            
            print(f"âœ… Fetched HTML ({len(response.text):,} characters)")
            return response.text
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            return None
    
    def extract_resources(self, html_content):
        """HTMLã‹ã‚‰ãƒªã‚½ãƒ¼ã‚¹ã‚’æŠ½å‡º"""
        resources = {
            'css': set(),
            'js': set(),
            'images': set(),
            'fonts': set(),
            'other': set()
        }
        
        # CSS
        for match in re.findall(r'<link[^>]+href=["\']([^"\']+\.css[^"\']*)["\']', html_content):
            resources['css'].add(urljoin(self.url, match))
        
        # JS
        for match in re.findall(r'<script[^>]+src=["\']([^"\']+)["\']', html_content):
            resources['js'].add(urljoin(self.url, match))
        
        # Images
        image_patterns = [
            r'<img[^>]+src=["\']([^"\']+)["\']',
            r'data-src=["\']([^"\']+)["\']',
            r'background-image:\s*url\(["\']?([^"\')\s]+)["\']?\)',
        ]
        for pattern in image_patterns:
            for match in re.findall(pattern, html_content):
                if not match.startswith('data:'):
                    resources['images'].add(urljoin(self.url, match))
        
        # Fonts
        for match in re.findall(r'["\']([^"\']+\.(woff2?|ttf|otf|eot))["\']', html_content):
            resources['fonts'].add(urljoin(self.url, match[0]))
        
        # çµ±è¨ˆã‚’è¡¨ç¤º
        total = sum(len(urls) for urls in resources.values())
        print(f"âœ… Found {total} resources:")
        for category, urls in resources.items():
            if urls:
                print(f"   - {category}: {len(urls)} files")
        
        # è¾æ›¸ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
        return {k: list(v) for k, v in resources.items()}
    
    def save_resource_list(self, resources):
        """ãƒªã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚’ä¿å­˜"""
        with open(os.path.join(self.output_dir, 'resources.json'), 'w') as f:
            json.dump(resources, f, indent=2)
    
    def download_all_resources(self, resources):
        """ã™ã¹ã¦ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        total = sum(len(urls) for urls in resources.values())
        current = 0
        
        for category, urls in resources.items():
            for url in urls:
                current += 1
                print(f"\r[{current}/{total}] Downloading: {os.path.basename(url)[:50]}...", end='')
                self.download_resource(url, category)
                time.sleep(0.1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        
        print(f"\nâœ… Downloaded {len(self.downloaded)} files")
        if self.failed:
            print(f"âš ï¸  Failed: {len(self.failed)} files")
    
    def download_resource(self, url, category):
        """å€‹åˆ¥ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        if url in self.downloaded:
            return
        
        try:
            local_path = self.url_to_local_path(url)
            full_path = os.path.join(self.output_dir, local_path)
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            os.makedirs(os.path.dirname(full_path), exist_ok=True)
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # ä¿å­˜
            with open(full_path, 'wb') as f:
                f.write(response.content)
            
            self.downloaded.add(url)
            self.stats[category] = self.stats.get(category, 0) + 1
            self.stats['total_size'] += len(response.content)
            
            # CSS/JSã®ä¾å­˜é–¢ä¿‚ã‚‚å‡¦ç†
            if category in ['css', 'js'] and response.text:
                self.process_dependencies(response.text, url, category)
                
        except Exception as e:
            self.failed.append((url, str(e)))
    
    def process_dependencies(self, content, base_url, file_type):
        """CSS/JSå†…ã®ä¾å­˜é–¢ä¿‚ã‚’å‡¦ç†"""
        if file_type == 'css':
            # @import
            for match in re.findall(r'@import\s+["\']([^"\']+)["\']', content):
                dep_url = urljoin(base_url, match)
                self.download_resource(dep_url, 'css')
            
            # url()
            for match in re.findall(r'url\(["\']?([^"\')\s]+)["\']?\)', content):
                if not match.startswith('data:'):
                    dep_url = urljoin(base_url, match)
                    category = self.get_resource_category(dep_url)
                    self.download_resource(dep_url, category)
    
    def get_resource_category(self, url):
        """URLã‹ã‚‰ãƒªã‚½ãƒ¼ã‚¹ã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®š"""
        ext = os.path.splitext(urlparse(url).path)[1].lower()
        
        if ext in ['.css']:
            return 'css'
        elif ext in ['.js']:
            return 'js'
        elif ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg', '.ico']:
            return 'images'
        elif ext in ['.woff', '.woff2', '.ttf', '.otf', '.eot']:
            return 'fonts'
        else:
            return 'other'
    
    def url_to_local_path(self, url):
        """URLã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›"""
        parsed = urlparse(url)
        
        # åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å ´åˆ
        if parsed.netloc == self.domain:
            path = parsed.path.lstrip('/')
            if parsed.query:
                # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã‚ã‚‹
                base, ext = os.path.splitext(path)
                path = f"{base}_{parsed.query}{ext}"
            return path
        
        # å¤–éƒ¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å ´åˆ
        else:
            domain_dir = parsed.netloc.replace(':', '_')
            path = parsed.path.lstrip('/')
            if parsed.query:
                base, ext = os.path.splitext(path)
                path = f"{base}_{parsed.query}{ext}"
            return os.path.join('external', domain_dir, path)
    
    def convert_paths(self, html_content):
        """HTMLãƒ‘ã‚¹ã‚’ç›¸å¯¾ãƒ‘ã‚¹ã«å¤‰æ›"""
        # è‡ªãƒ‰ãƒ¡ã‚¤ãƒ³ã®URLã‚’ç›¸å¯¾ãƒ‘ã‚¹ã«
        patterns = [
            (f'https://{self.domain}/', './'),
            (f'http://{self.domain}/', './'),
            (f'//{self.domain}/', './')
        ]
        
        for pattern, replacement in patterns:
            html_content = html_content.replace(pattern, replacement)
        
        # çµ¶å¯¾ãƒ‘ã‚¹ã‚’ç›¸å¯¾ãƒ‘ã‚¹ã«
        html_content = re.sub(r'(href|src|data-src)="/([^"]+)"', r'\1="./\2"', html_content)
        html_content = re.sub(r"(href|src|data-src)='/([^']+)'", r"\1='./\2'", html_content)
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã®ä¿®æ­£
        def fix_query_params(match):
            attr = match.group(1)
            path = match.group(2)
            ext = match.group(3)
            query = match.group(4)
            
            if path.startswith('./'):
                return f'{attr}="{path}_{query}{ext}"'
            return match.group(0)
        
        # CSS/JSãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿®æ­£
        html_content = re.sub(
            r'(href|src)="([^"]+)(\.(?:css|js))\?([^"]+)"',
            fix_query_params,
            html_content
        )
        
        return html_content
    
    def fix_missing_resources(self):
        """å¤±æ•—ã—ãŸãƒªã‚½ãƒ¼ã‚¹ã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ"""
        if not self.failed:
            return
        
        # é€æ˜ãª1x1ç”»åƒ
        transparent_png = b'\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00\x01\x00\x00\x00\x01\x08\x06\x00\x00\x00\x1f\x15\xc4\x89\x00\x00\x00\rIDATx\xdac\xf8\x0f\x00\x00\x01\x01\x00\x05\x00\x00\x00\x00IEND\xaeB`\x82'
        
        for url, error in self.failed:
            local_path = self.url_to_local_path(url)
            full_path = os.path.join(self.output_dir, local_path)
            
            # ç”»åƒã®å ´åˆã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
            if self.get_resource_category(url) == 'images':
                os.makedirs(os.path.dirname(full_path), exist_ok=True)
                with open(full_path, 'wb') as f:
                    f.write(transparent_png)
        
        # å¤±æ•—ãƒªã‚¹ãƒˆã‚’ä¿å­˜
        with open(os.path.join(self.output_dir, 'failed_downloads.txt'), 'w') as f:
            for url, error in self.failed:
                f.write(f"{url} - {error}\n")
    
    def staticize_content(self, html_content):
        """å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™çš„åŒ–"""
        # Google Tag Managerç­‰ã‚’ç„¡åŠ¹åŒ–
        html_content = re.sub(
            r'<!-- Google Tag Manager -->.*?<!-- End Google Tag Manager -->',
            '<!-- Analytics disabled for offline use -->',
            html_content,
            flags=re.DOTALL
        )
        
        # Font Awesome Kitã‚’ç„¡åŠ¹åŒ–
        html_content = re.sub(
            r'<script src="https://kit\.fontawesome\.com/[^"]+"></script>',
            '<!-- Font Awesome Kit disabled for offline use -->',
            html_content
        )
        
        return html_content
    
    def save_html(self, html_content):
        """æœ€çµ‚çš„ãªHTMLã‚’ä¿å­˜"""
        output_file = os.path.join(self.output_dir, 'index.html')
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"âœ… Saved to {output_file}")
    
    def show_results(self):
        """çµæœã‚’è¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ğŸ“Š REPLICATION RESULTS")
        print("=" * 60)
        
        print(f"\nâœ… Successfully replicated: {self.url}")
        print(f"ğŸ“ Output directory: {self.output_dir}")
        print(f"ğŸ“¦ Total size: {self.stats['total_size'] / 1024 / 1024:.2f} MB")
        
        print("\nğŸ“ˆ File statistics:")
        for category, count in self.stats.items():
            if category != 'total_size' and count > 0:
                print(f"   - {category}: {count} files")
        
        if self.failed:
            print(f"\nâš ï¸  Failed downloads: {len(self.failed)}")
            print("   (See failed_downloads.txt for details)")
    
    def create_helper_files(self):
        """ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        # README
        readme_content = f"""# Website Replica - {self.domain}

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Original URL: {self.url}

## ğŸ“ Contents

- `index.html` - Main entry point
- `resources.json` - List of all resources
- `failed_downloads.txt` - Failed downloads (if any)

## ğŸš€ Usage

1. **Direct Access**: Open `index.html` in your browser
2. **Local Server** (recommended):
   ```bash
   python3 -m http.server 8000
   # Open http://localhost:8000 in your browser
   ```

## ğŸ“Š Statistics

- Total files: {len(self.downloaded)}
- Total size: {self.stats['total_size'] / 1024 / 1024:.2f} MB
- Failed downloads: {len(self.failed)}

## âš ï¸ Notes

- Some dynamic features may not work offline
- External resources (CDN) require internet connection
- JavaScript functionality may be limited when opened directly
"""
        
        with open(os.path.join(self.output_dir, 'README.md'), 'w') as f:
            f.write(readme_content)
        
        # ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
        server_script = '''#!/usr/bin/env python3
import http.server
import socketserver
import os

PORT = 8000

os.chdir(os.path.dirname(os.path.abspath(__file__)))

Handler = http.server.SimpleHTTPRequestHandler

with socketserver.TCPServer(("", PORT), Handler) as httpd:
    print(f"Server running at http://localhost:{PORT}")
    print("Press Ctrl+C to stop")
    httpd.serve_forever()
'''
        
        server_file = os.path.join(self.output_dir, 'start_server.py')
        with open(server_file, 'w') as f:
            f.write(server_script)
        os.chmod(server_file, 0o755)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='Website Replicator - Create offline copies of websites',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  python3 website_replicator.py https://example.com
  python3 website_replicator.py https://example.com -o my_replica
  python3 website_replicator.py https://example.com --include-external
        '''
    )
    
    parser.add_argument('url', help='URL of the website to replicate')
    parser.add_argument('-o', '--output', help='Output directory name')
    parser.add_argument('--include-external', action='store_true', 
                        help='Include external resources (CDN, etc.)')
    
    args = parser.parse_args()
    
    # ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚¿ãƒ¼ã‚’å®Ÿè¡Œ
    replicator = WebsiteReplicator(args.url, args.output)
    success = replicator.replicate()
    
    if success:
        print(f"\nâœ¨ Success! Your replica is ready in: {replicator.output_dir}")
        print(f"ğŸŒ To view: open {os.path.join(replicator.output_dir, 'index.html')}")
    else:
        print("\nâŒ Replication failed!")
        sys.exit(1)


if __name__ == "__main__":
    main()